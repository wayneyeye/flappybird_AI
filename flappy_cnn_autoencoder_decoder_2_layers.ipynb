{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "snapshot_array = pickle.load( open( \"snapshot/np_128square_without_label.pickle\", \"rb\" ) )\n",
    "snapshot_array=snapshot_array/255.\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "snapshot_array=shuffle(snapshot_array)\n",
    "training_ratio=0.7\n",
    "training_cut=int(training_ratio*snapshot_array.shape[0])\n",
    "X_train=snapshot_array[:training_cut,]\n",
    "X_test=snapshot_array[-1000:,]\n",
    "\n",
    "del snapshot_array\n",
    "\n",
    "def convpool(X,W,b):\n",
    "    conv_out=tf.nn.conv2d(X,W,strides=[1,1,1,1],padding=\"SAME\")\n",
    "    conv_out=tf.nn.bias_add(conv_out,b)\n",
    "    conv_out=tf.nn.elu(conv_out)\n",
    "    pool_out=tf.nn.max_pool(conv_out,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "    return pool_out\n",
    "\n",
    "def init_filter(shape,poolsz):\n",
    "    w=np.random.randn(*shape)/np.sqrt(np.prod(shape[:-1])+shape[-1]*np.prod(shape[:-2]/np.prod(poolsz)))\n",
    "    return w.astype(np.float32)\n",
    "\n",
    "from functools import partial\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer(mode=\"FAN_AVG\") # he init method\n",
    "scale=0.0\n",
    "my_dense=partial(tf.layers.dense,activation=tf.nn.elu,\n",
    "                 kernel_regularizer=tf.contrib.layers.l1_regularizer(scale),\n",
    "                 kernel_initializer=he_init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_savepath=\"model_checkpoints_large/CAE_04302018_2layers_xsmall.ckpt\"\n",
    "n_epochs=200\n",
    "N=X_train.shape[0]\n",
    "batch_size=500\n",
    "n_batches=N//batch_size\n",
    "pool_sz=(2,2)\n",
    "\n",
    "learning_rate=0.01\n",
    "tf.reset_default_graph()\n",
    "# cnn_pool layer 1\n",
    "W1_shape=(4,4,3,10)\n",
    "W1_init=init_filter(W1_shape,pool_sz)\n",
    "b1_init=np.zeros(W1_shape[-1],dtype=np.float32)\n",
    "\n",
    "# cnn_pool layer 2\n",
    "W2_shape=(4,4,10,3)\n",
    "W2_init=init_filter(W2_shape,pool_sz)\n",
    "b2_init=np.zeros(W2_shape[-1],dtype=np.float32)\n",
    "\n",
    "\n",
    "X=tf.placeholder(tf.float32,shape=(None,128,128,3),name=\"X\")\n",
    "\n",
    "with tf.name_scope(\"cnn\"):\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        W1=tf.Variable(W1_init.astype(np.float32))\n",
    "        b1=tf.Variable(b1_init.astype(np.float32))\n",
    "        W2=tf.Variable(W2_init.astype(np.float32))\n",
    "        b2=tf.Variable(b2_init.astype(np.float32))\n",
    "\n",
    "        Z1=convpool(X,W1,b1)\n",
    "        Z2=convpool(Z1,W2,b2)\n",
    "\n",
    "    with tf.device(\"/gpu:1\"):\n",
    "        Z4=tf.layers.conv2d_transpose(Z2,filters=10,kernel_size=(4,4),strides=(2, 2),\n",
    "                                      padding='same',activation=tf.nn.elu)\n",
    "        Z5=tf.layers.conv2d_transpose(Z4,filters=3,kernel_size=(4,4),strides=(2, 2),\n",
    "                                      padding='same',activation=tf.nn.elu)\n",
    "        reconstruction=Z5\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(tf.squared_difference(reconstruction, X))\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op=optimizer.minimize(loss)\n",
    "with tf.name_scope(\"saver\"):\n",
    "    saver = tf.train.Saver()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model_checkpoints_large/CAE_04302018_2layers_xsmall.ckpt\n",
      "restoring error, will start over!\n",
      "epoch  95 MSE(training_batch) 0.002667 MSE(testing) 0.002673\r"
     ]
    }
   ],
   "source": [
    "init=tf.global_variables_initializer()\n",
    "train_mse_save=[]\n",
    "test_mse_save=[]\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    try:\n",
    "        saver.restore(sess, checkpoints_savepath)\n",
    "    except:\n",
    "        print(\"restoring error, will start over!\")\n",
    "    for epoch in range(1,n_epochs+1):\n",
    "        X_train=shuffle(X_train)\n",
    "        for batch in range(n_batches):\n",
    "            X_batch=X_train[batch*batch_size:batch*batch_size+batch_size,]\n",
    "            training_loss=loss.eval(feed_dict={X:X_batch})\n",
    "            sess.run(training_op,feed_dict={X:X_batch})\n",
    "            testing_loss=loss.eval(feed_dict={X:X_test})\n",
    "        print(\"epoch {:3d} MSE(training_batch) {:7.6f} MSE(testing) {:7.6f}\".\\\n",
    "              format(epoch,training_loss,testing_loss),end=\"\\r\")\n",
    "        train_mse_save.append(training_loss)\n",
    "        test_mse_save.append(testing_loss)\n",
    "        if epoch % 10==0:\n",
    "            save_path=saver.save(sess,checkpoints_savepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_train,=plt.plot(train_mse_save,label=\"training\")\n",
    "line_test,=plt.plot(test_mse_save,label=\"testing\")\n",
    "plt.legend(handles=[line_train, line_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize the reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    saver.restore(sess, checkpoints_savepath)\n",
    "    n_image_row=4\n",
    "    n_image_col=2\n",
    "    total_image=n_image_row*n_image_col\n",
    "    image_index=random.sample(list(range(X_test.shape[0])),n_image_row)\n",
    "    for i,k in enumerate(image_index):\n",
    "        plt.subplot(n_image_row,n_image_col,i*n_image_col+1)\n",
    "        plt.imshow(X_test[k])\n",
    "        plt.subplot(n_image_row,n_image_col,i*n_image_col+2)\n",
    "        reconstructed_image=reconstruction.eval(feed_dict={X:[X_test[k]]})\n",
    "        plt.imshow(reconstructed_image[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize the output from the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    saver.restore(sess, checkpoints_savepath)\n",
    "    n_image_row=5\n",
    "    n_image_col=2\n",
    "    total_image=n_image_row*n_image_col\n",
    "    image_index=list(range(10))\n",
    "    for i,k in enumerate(image_index):\n",
    "        plt.subplot(n_image_row,n_image_col,i+1)\n",
    "        encoded_image=Z1.eval(feed_dict={X:[X_test[0]]})\n",
    "        plt.imshow(encoded_image[0,:,:,k],cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    saver.restore(sess, checkpoints_savepath)\n",
    "    n_image_row=2\n",
    "    n_image_col=2\n",
    "    total_image=n_image_row*n_image_col\n",
    "    image_index=list(range(3))\n",
    "    for i,k in enumerate(image_index):\n",
    "        plt.subplot(n_image_row,n_image_col,i+1)\n",
    "        encoded_image=Z2.eval(feed_dict={X:[X_test[200]]})\n",
    "        plt.imshow(encoded_image[0,:,:,k],cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize the filters (first layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    saver.restore(sess, checkpoints_savepath)\n",
    "    n_image_row=4\n",
    "    n_image_col=4\n",
    "    total_image=n_image_row*n_image_col\n",
    "    image_index=list(range(10))\n",
    "    for i,k in enumerate(image_index):\n",
    "        plt.subplot(n_image_row,n_image_col,i+1)\n",
    "        filter1=W1.eval()\n",
    "        plt.imshow(filter1[:,:,:,k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

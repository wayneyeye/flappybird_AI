{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "snapshot_array = pickle.load( open( \"snapshot/np_without_label.pickle\", \"rb\" ) )\n",
    "snapshot_array=snapshot_array/255.\n",
    "\n",
    "# n_image_row=3\n",
    "# n_image_col=3\n",
    "# total_image=n_image_row*n_image_col\n",
    "# image_index=random.sample(list(range(snapshot_array.shape[0])),total_image)\n",
    "# for i,k in enumerate(image_index):\n",
    "#     plt.subplot(n_image_row,n_image_col,i+1)\n",
    "#     plt.imshow(snapshot_array[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "snapshot_array=shuffle(snapshot_array)\n",
    "training_ratio=0.1\n",
    "training_cut=int(training_ratio*snapshot_array.shape[0])\n",
    "X_train=snapshot_array[:training_cut,]\n",
    "X_test=snapshot_array[training_cut:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "del snapshot_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n"
     ]
    }
   ],
   "source": [
    "def convpool(X,W,b):\n",
    "    conv_out=tf.nn.conv2d(X,W,strides=[1,1,1,1],padding=\"SAME\")\n",
    "    conv_out=tf.nn.bias_add(conv_out,b)\n",
    "    conv_out=tf.nn.elu(conv_out)\n",
    "    pool_out=tf.nn.max_pool(conv_out,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "    return pool_out\n",
    "\n",
    "def init_filter(shape,poolsz):\n",
    "    w=np.random.randn(*shape)/np.sqrt(np.prod(shape[:-1])+shape[-1]*np.prod(shape[:-2]/np.prod(poolsz)))\n",
    "    return w.astype(np.float32)\n",
    "\n",
    "from functools import partial\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer(mode=\"FAN_AVG\") # he init method\n",
    "scale=0.0\n",
    "my_dense=partial(tf.layers.dense,activation=tf.nn.elu,\n",
    "                 kernel_regularizer=tf.contrib.layers.l1_regularizer(scale),\n",
    "                 kernel_initializer=he_init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_savepath=\"model_checkpoints_large/CAE_04292018_1.ckpt\"\n",
    "n_epochs=50\n",
    "N=X_train.shape[0]\n",
    "batch_size=100\n",
    "n_batches=N//batch_size\n",
    "pool_sz=(2,2)\n",
    "\n",
    "learning_rate=0.001\n",
    "tf.reset_default_graph()\n",
    "# cnn_pool layer 1\n",
    "W1_shape=(3,3,3,20)\n",
    "W1_init=init_filter(W1_shape,pool_sz)\n",
    "b1_init=np.zeros(W1_shape[-1],dtype=np.float32)\n",
    "\n",
    "# cnn_pool layer 2\n",
    "W2_shape=(3,3,20,50)\n",
    "W2_init=init_filter(W2_shape,pool_sz)\n",
    "b2_init=np.zeros(W2_shape[-1],dtype=np.float32)\n",
    "\n",
    "# cnn_transpose layer\n",
    "W3t_shape=(3,3,3,50)\n",
    "W3t_init=init_filter(W3t_shape,(1,1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tf.placeholder(tf.float32,shape=(None,410,288,3),name=\"X\")\n",
    "with tf.name_scope(\"cnn\"):\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        W1=tf.Variable(W1_init.astype(np.float32))\n",
    "        b1=tf.Variable(b1_init.astype(np.float32))\n",
    "        W2=tf.Variable(W2_init.astype(np.float32))\n",
    "        b2=tf.Variable(b2_init.astype(np.float32))\n",
    "        W3t=tf.Variable(W3t_init.astype(np.float32))\n",
    "        Z1=convpool(X,W1,b1)\n",
    "        Z2=convpool(Z1,W2,b2)\n",
    "    with tf.device(\"/gpu:1\"):\n",
    "        Z3=tf.layers.conv2d_transpose(Z2,filters=20,kernel_size=(3,3),strides=(2, 2),\n",
    "                                      padding='same',activation=tf.nn.elu)\n",
    "        Z4=tf.layers.conv2d_transpose(Z3,filters=3,kernel_size=(3,3),strides=(2, 2),\n",
    "                                      padding='same',activation=tf.nn.elu)\n",
    "        reconstruction=Z4[:,:410,:,:]\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(tf.squared_difference(reconstruction, X))\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op=optimizer.minimize(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  49 MSE 0.002769\r"
     ]
    }
   ],
   "source": [
    "init=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        X_train=shuffle(X_train)\n",
    "        for batch in range(n_batches):\n",
    "            X_batch=X_train[batch*batch_size:batch*batch_size+batch_size,]\n",
    "            sess.run(training_op,feed_dict={X:X_batch})\n",
    "        print(\"epoch {:3d} MSE {:7.6f}\".format(epoch,loss.eval(feed_dict={X:X_batch})),end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden1=500\n",
    "n_outputs=10\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    dropout_rate=0.2\n",
    "    training=tf.placeholder_with_default(False,shape=(),name=\"training\")\n",
    "    \n",
    "    Z2_f=tf.contrib.layers.flatten(Z2)\n",
    "    \n",
    "    hidden1=my_dense(Z2_f,n_hidden1,name=\"hidden1\",kernel_initializer=he_init,\n",
    "                     activation=tf.nn.elu)\n",
    "    \n",
    "    hidden1_drop=tf.layers.dropout(hidden1,dropout_rate,training=training)\n",
    "    \n",
    "    logits=my_dense(hidden1_drop,n_outputs, kernel_regularizer=tf.contrib.layers.l1_regularizer(scale),\n",
    "                 kernel_initializer=he_init,name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy=tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "    loss=tf.reduce_mean(xentropy,name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer=tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
    "    training_op=optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct=tf.nn.in_top_k(logits,y,1)\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct,tf.float32))\n",
    "    \n",
    "init=tf.global_variables_initializer()\n",
    "saver=tf.train.Saver()\n",
    "train_accuracy_save=[]\n",
    "test_accuracy_save=[]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        Xtrain,Ytrain=shuffle(Xtrain,Ytrain)\n",
    "        for iteration in range(N//batch_sz):\n",
    "            X_batch,y_batch=(Xtrain[iteration*batch_sz:(iteration*batch_sz+batch_sz),:],\n",
    "                             Ytrain[iteration*batch_sz:(iteration*batch_sz+batch_sz)])\n",
    "            sess.run(training_op,feed_dict={X:X_batch,y:y_batch,training:True}) # dropout=True\n",
    "#         print(logits.eval(feed_dict={X:X_batch,y:y_batch}))\n",
    "        acc_train=accuracy.eval(feed_dict={X:X_batch,y:y_batch}) # dropout=True\n",
    "        acc_test=accuracy.eval(feed_dict={X:Xtest,y:Ytest})\n",
    "        clear_output()\n",
    "        print(epoch+1,\"Train accuracy: \",acc_train,\" Test accuracy: \",acc_test,end=\"\\r\")\n",
    "        train_accuracy_save.append(acc_train)\n",
    "        test_accuracy_save.append(acc_test)\n",
    "    save_path=saver.save(sess,checkpoints_savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
